{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist-dcgan.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPZdLKrBcL7hMkL2Y8F+rwo"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"90JI2Lpc2RaI","executionInfo":{"status":"ok","timestamp":1603767527581,"user_tz":240,"elapsed":3913,"user":{"displayName":"e e","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRFShoYRsDhZnpGvDhrdiTr-lcKIMYOM41EgrP=s64","userId":"09974983085928519513"}}},"source":["import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","# from model_utils import Discriminator, Generator # inspired from DCGAN paper"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"ccv4Tpz790fc","executionInfo":{"status":"ok","timestamp":1603767528574,"user_tz":240,"elapsed":359,"user":{"displayName":"e e","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRFShoYRsDhZnpGvDhrdiTr-lcKIMYOM41EgrP=s64","userId":"09974983085928519513"}}},"source":["class Discriminator(nn.Module):\n","    def __init__(self, channels_img, features_d):\n","        super(Discriminator, self).__init__()\n","        self.net = nn.Sequential(\n","            # N x features_d x 64 x 64\n","            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(features_d, features_d*2, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(features_d*2),\n","            nn.LeakyReLU(0.2),\n","            # N x features_d x 32 x 32\n","            nn.Conv2d(features_d*2, features_d*4, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(features_d*4),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(features_d*4, features_d*8, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(features_d*8),\n","            nn.LeakyReLU(0.2),\n","            # N x features*8 x 4 x 4\n","            nn.Conv2d(features_d*8, 1, kernel_size=4, stride=2, padding=0),\n","            # N x 1 x 1 x 1\n","            nn.Sigmoid()\n","        )\n","        \n","\n","    def forward(self, x):\n","        return self.net(x)\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_31FG1tM_yr","executionInfo":{"status":"ok","timestamp":1603767529974,"user_tz":240,"elapsed":397,"user":{"displayName":"e e","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRFShoYRsDhZnpGvDhrdiTr-lcKIMYOM41EgrP=s64","userId":"09974983085928519513"}}},"source":["class Generator(nn.Module):\n","    # param channels_noise: number of channels of starting image noise to begin generation from\n","    # param channels_img: number of channels of final image being generated\n","    # param features_g:\n","    def __init__(self, channels_noise, channels_img, features_g):\n","        super(Generator, self).__init__()\n","\n","        self.net = nn.Sequential(\n","            # N x channels_noise x 1 x 1\n","            nn.ConvTranspose2d(channels_noise, features_g*16, kernel_size=4, stride=1, padding=0),\n","            nn.BatchNorm2d(features_g*16),\n","            nn.ReLU(),\n","            # N x features_g*16 x 4 x 4\n","            nn.ConvTranspose2d(features_g*16, features_g*8, kernel_size=4, stride=2, padding=1), # stride = 2 doubles the size of the input\n","            nn.BatchNorm2d(features_g*8),\n","            nn.ReLU(),\n","\n","            nn.ConvTranspose2d(features_g*8, features_g*4, kernel_size=4, stride=2, padding=1), # stride = 2 doubles the size of the input\n","            nn.BatchNorm2d(features_g*4),\n","            nn.ReLU(),\n","\n","            nn.ConvTranspose2d(features_g*4, features_g*2, kernel_size=4, stride=2, padding=1), # stride = 2 doubles the size of the input\n","            nn.BatchNorm2d(features_g*2),\n","            nn.ReLU(),\n","\n","            nn.ConvTranspose2d(features_g*2, channels_img, kernel_size=4, stride=2, padding=1),\n","            # N x channels_img x 64 x 64\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"N_sx2huOPncD","executionInfo":{"status":"ok","timestamp":1603767929154,"user_tz":240,"elapsed":274571,"user":{"displayName":"e e","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRFShoYRsDhZnpGvDhrdiTr-lcKIMYOM41EgrP=s64","userId":"09974983085928519513"}},"outputId":"cf99df04-faab-48f1-f0c6-9534c0fbff85","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# hyperparameters\n","LR = 0.0002\n","BS = 64\n","IMAGE_SIZE = 64 # MNIST (28x28) -> 64x64\n","CHANNELS_IMG = 1 # MNIST grayscale numbers, 1 channel.\n","CHANNELS_NOISE = 256\n","EPOCHS = 10\n","r_label = 1\n","f_label = 0\n","\n","# paper does 64, larger network, not needed for MNIST, use for face gen\n","features_d = 16\n","features_g = 16\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Running on GPU\" if torch.cuda.is_available() else print(\"Running on CPU (SLOW)\"))\n","\n","my_transforms = transforms.Compose([\n","    transforms.Resize(IMAGE_SIZE),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,)),\n","])\n","dataset = datasets.MNIST(root='dataset/', train=True, transform=my_transforms, download=True)\n","dataloader = DataLoader(dataset, batch_size=BS, shuffle=True)\n","num_batches = len(dataloader)\n","print(f\"Batches: {num_batches}\")\n","\n","fixed_noise = torch.randn(64, CHANNELS_NOISE, 1, 1).to(device) # used as test input as network improves\n","\n","loss_function = nn.BCELoss()\n","\n","class Net():\n","\n","    # validation tensorboard\n","    writer_real = SummaryWriter(f\"runs/GAN_MNIST/test_real\")\n","    writer_fake = SummaryWriter(f\"runs/GAN_MNIST/test_fake\")\n","\n","    def main(self):\n","        self.Dnet = Discriminator(CHANNELS_IMG, features_d).to(device)\n","        self.Gnet = Generator(CHANNELS_NOISE, CHANNELS_IMG, features_g).to(device)\n","        self.optimizerD = optim.Adam(self.Dnet.parameters(), lr=LR, betas=(0.5, 0.9999))\n","        self.optimizerG = optim.Adam(self.Gnet.parameters(), lr=LR, betas=(0.5, 0.9999))\n","        self.train()\n","\n","    def train(self):\n","        for epoch in range(EPOCHS):\n","            for batch_idx, (data, targets) in enumerate(dataloader):\n","                data = data.to(device)\n","                batch_size = data.shape[0]\n","\n","                # train discriminator: max log[D(x)] + log(1 - D[G(z)])\n","\n","                # train discrimintator on real MNIST data, classify as real.\n","                self.Dnet.zero_grad()\n","                label = (torch.ones(batch_size)*0.9).to(device) # labels for training all REAL images, 1 label.\n","                output = self.Dnet(data).reshape(-1) # pass REAL images\n","                output = output[:64]\n","                lossD_real = loss_function(output, label)\n","                D_x = output.mean().item() # mean prediction for all images per batch\n","\n","                # generate fake Gnet images, train discrimintaor on fakes to classify as fakes.\n","                noise = torch.randn(batch_size, CHANNELS_NOISE, 1, 1).to(device)\n","                fake = self.Gnet(noise).to(device) # outputs generated fakes\n","\n","                label = (torch.ones(batch_size)*0.1).to(device) # fake-distinguishing labels\n","                output = self.Dnet(fake.detach()).reshape(-1) # detaching fake images from gradients, stops generator from training on fake data.\n","                output = output[:64]\n","                lossD_fake = loss_function(output, label)\n","\n","                # combine (real image loss) + (fake image loss) and backpropagate, train discriminator.\n","                lossD = lossD_real + lossD_fake\n","                lossD.backward()\n","                self.optimizerD.step()\n","\n","                # train generator: max log[D(G(z))]\n","                self.Gnet.zero_grad()\n","                label = torch.ones(batch_size).to(device) # no generalization multiplication, generator wants to be as accurate as it can be.\n","                output = self.Dnet(fake).reshape(-1) # get prediction from descriminator\n","                output = output[:64]\n","                lossG = loss_function(output, label)\n","                lossG.backward()\n","                self.optimizerG.step() # take answers from discriminator, apply to improve generator.\n","\n","                if batch_idx % 100 == 0:\n","                    # every 100 steps\n","                    print(f\"[e{epoch}/{EPOCHS}] [b{batch_idx}/{num_batches}] lossD: {lossD:.4f}, lossG: {lossG:.4f}, D(x): {D_x:.4f}\")\n","\n","                    with torch.no_grad():\n","                        fake = self.Gnet(fixed_noise)\n","                        img_grid_real = torchvision.utils.make_grid(data[:32], normalize=True)\n","                        img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n","                        self.writer_real.add_image(\"MNIST Real Images\", img_grid_real)\n","                        self.writer_fake.add_image(\"MNIST Fake Images\", img_grid_fake)\n","\n","\n","if __name__ == \"__main__\":\n","    net = Net()\n","    net.main()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Running on GPU\n","Batches: 938\n","[e0/10] [b0/938] lossD: 1.4448, lossG: 0.8309, D(x): 0.5351\n","[e0/10] [b100/938] lossD: 0.6863, lossG: 2.4938, D(x): 0.8328\n","[e0/10] [b200/938] lossD: 0.7002, lossG: 3.4931, D(x): 0.8649\n","[e0/10] [b300/938] lossD: 0.9801, lossG: 2.3773, D(x): 0.7922\n","[e0/10] [b400/938] lossD: 0.8193, lossG: 2.1091, D(x): 0.8244\n","[e0/10] [b500/938] lossD: 0.8174, lossG: 1.4320, D(x): 0.7110\n","[e0/10] [b600/938] lossD: 0.7788, lossG: 1.6278, D(x): 0.8347\n","[e0/10] [b700/938] lossD: 0.9507, lossG: 1.8831, D(x): 0.8843\n","[e0/10] [b800/938] lossD: 0.7929, lossG: 1.6713, D(x): 0.8093\n","[e0/10] [b900/938] lossD: 0.8150, lossG: 1.0476, D(x): 0.7167\n","[e1/10] [b0/938] lossD: 0.8399, lossG: 1.5816, D(x): 0.7269\n","[e1/10] [b100/938] lossD: 0.7721, lossG: 1.7401, D(x): 0.8041\n","[e1/10] [b200/938] lossD: 0.9087, lossG: 2.1355, D(x): 0.8711\n","[e1/10] [b300/938] lossD: 0.8008, lossG: 1.7515, D(x): 0.7953\n","[e1/10] [b400/938] lossD: 0.9490, lossG: 1.1562, D(x): 0.6901\n","[e1/10] [b500/938] lossD: 0.8305, lossG: 2.6620, D(x): 0.8634\n","[e1/10] [b600/938] lossD: 0.7948, lossG: 1.9377, D(x): 0.7101\n","[e1/10] [b700/938] lossD: 0.7541, lossG: 2.1198, D(x): 0.8697\n","[e1/10] [b800/938] lossD: 0.8515, lossG: 1.0633, D(x): 0.6491\n","[e1/10] [b900/938] lossD: 0.8914, lossG: 0.9217, D(x): 0.6211\n","[e2/10] [b0/938] lossD: 0.7642, lossG: 1.6494, D(x): 0.7712\n","[e2/10] [b100/938] lossD: 0.8668, lossG: 1.2800, D(x): 0.6579\n","[e2/10] [b200/938] lossD: 0.9078, lossG: 1.3077, D(x): 0.8056\n","[e2/10] [b300/938] lossD: 1.1293, lossG: 4.4599, D(x): 0.9093\n","[e2/10] [b400/938] lossD: 0.8125, lossG: 1.1429, D(x): 0.6991\n","[e2/10] [b500/938] lossD: 0.8197, lossG: 1.5590, D(x): 0.6790\n","[e2/10] [b600/938] lossD: 0.8113, lossG: 2.1318, D(x): 0.8588\n","[e2/10] [b700/938] lossD: 0.7761, lossG: 1.6721, D(x): 0.8050\n","[e2/10] [b800/938] lossD: 0.8982, lossG: 1.3333, D(x): 0.6175\n","[e2/10] [b900/938] lossD: 1.0294, lossG: 0.9677, D(x): 0.5237\n","[e3/10] [b0/938] lossD: 0.9648, lossG: 1.0357, D(x): 0.5835\n","[e3/10] [b100/938] lossD: 0.8611, lossG: 0.9792, D(x): 0.6447\n","[e3/10] [b200/938] lossD: 0.7308, lossG: 1.6578, D(x): 0.7882\n","[e3/10] [b300/938] lossD: 0.8188, lossG: 1.4530, D(x): 0.7132\n","[e3/10] [b400/938] lossD: 0.8609, lossG: 2.6105, D(x): 0.8850\n","[e3/10] [b500/938] lossD: 0.7909, lossG: 1.8281, D(x): 0.8185\n","[e3/10] [b600/938] lossD: 0.8451, lossG: 1.7085, D(x): 0.7589\n","[e3/10] [b700/938] lossD: 0.9658, lossG: 0.9633, D(x): 0.5807\n","[e3/10] [b800/938] lossD: 0.8724, lossG: 1.2486, D(x): 0.6364\n","[e3/10] [b900/938] lossD: 0.8348, lossG: 1.3922, D(x): 0.6776\n","[e4/10] [b0/938] lossD: 0.7812, lossG: 1.6260, D(x): 0.7924\n","[e4/10] [b100/938] lossD: 0.7804, lossG: 2.0673, D(x): 0.8307\n","[e4/10] [b200/938] lossD: 0.8808, lossG: 2.4942, D(x): 0.8567\n","[e4/10] [b300/938] lossD: 0.8449, lossG: 1.8963, D(x): 0.8282\n","[e4/10] [b400/938] lossD: 0.7387, lossG: 2.4666, D(x): 0.8535\n","[e4/10] [b500/938] lossD: 0.8013, lossG: 1.6762, D(x): 0.7210\n","[e4/10] [b600/938] lossD: 0.7642, lossG: 1.6125, D(x): 0.7764\n","[e4/10] [b700/938] lossD: 0.7539, lossG: 1.9663, D(x): 0.7634\n","[e4/10] [b800/938] lossD: 0.8037, lossG: 2.0695, D(x): 0.8653\n","[e4/10] [b900/938] lossD: 0.7417, lossG: 1.9481, D(x): 0.7617\n","[e5/10] [b0/938] lossD: 0.7539, lossG: 1.7971, D(x): 0.7718\n","[e5/10] [b100/938] lossD: 0.7625, lossG: 1.5210, D(x): 0.7389\n","[e5/10] [b200/938] lossD: 0.8383, lossG: 3.2659, D(x): 0.8959\n","[e5/10] [b300/938] lossD: 0.7320, lossG: 2.0480, D(x): 0.8098\n","[e5/10] [b400/938] lossD: 0.7857, lossG: 2.3898, D(x): 0.8445\n","[e5/10] [b500/938] lossD: 0.9553, lossG: 0.6913, D(x): 0.5751\n","[e5/10] [b600/938] lossD: 0.7215, lossG: 1.5277, D(x): 0.7875\n","[e5/10] [b700/938] lossD: 0.6893, lossG: 2.3374, D(x): 0.8730\n","[e5/10] [b800/938] lossD: 0.8012, lossG: 1.4861, D(x): 0.7160\n","[e5/10] [b900/938] lossD: 0.8912, lossG: 1.8312, D(x): 0.8697\n","[e6/10] [b0/938] lossD: 1.0746, lossG: 1.0458, D(x): 0.5129\n","[e6/10] [b100/938] lossD: 0.7476, lossG: 1.9498, D(x): 0.8472\n","[e6/10] [b200/938] lossD: 0.7563, lossG: 2.3344, D(x): 0.8866\n","[e6/10] [b300/938] lossD: 0.7507, lossG: 1.5558, D(x): 0.7661\n","[e6/10] [b400/938] lossD: 0.6973, lossG: 2.5274, D(x): 0.9277\n","[e6/10] [b500/938] lossD: 2.2565, lossG: 1.5020, D(x): 0.9506\n","[e6/10] [b600/938] lossD: 0.7349, lossG: 1.6307, D(x): 0.7696\n","[e6/10] [b700/938] lossD: 0.7482, lossG: 1.8959, D(x): 0.8100\n","[e6/10] [b800/938] lossD: 0.7191, lossG: 2.0261, D(x): 0.7922\n","[e6/10] [b900/938] lossD: 0.7246, lossG: 1.7683, D(x): 0.7939\n","[e7/10] [b0/938] lossD: 0.7666, lossG: 2.2091, D(x): 0.8198\n","[e7/10] [b100/938] lossD: 0.7508, lossG: 1.2692, D(x): 0.7526\n","[e7/10] [b200/938] lossD: 0.8229, lossG: 2.2231, D(x): 0.8920\n","[e7/10] [b300/938] lossD: 0.7075, lossG: 1.9554, D(x): 0.8065\n","[e7/10] [b400/938] lossD: 0.7552, lossG: 2.3713, D(x): 0.8214\n","[e7/10] [b500/938] lossD: 0.7489, lossG: 2.1618, D(x): 0.8882\n","[e7/10] [b600/938] lossD: 0.7673, lossG: 1.7274, D(x): 0.8064\n","[e7/10] [b700/938] lossD: 0.7488, lossG: 1.4076, D(x): 0.7514\n","[e7/10] [b800/938] lossD: 0.7135, lossG: 1.5661, D(x): 0.8158\n","[e7/10] [b900/938] lossD: 0.7626, lossG: 1.5100, D(x): 0.7380\n","[e8/10] [b0/938] lossD: 0.6864, lossG: 1.8615, D(x): 0.8387\n","[e8/10] [b100/938] lossD: 0.6928, lossG: 1.7056, D(x): 0.8333\n","[e8/10] [b200/938] lossD: 0.7588, lossG: 2.7363, D(x): 0.9279\n","[e8/10] [b300/938] lossD: 0.7263, lossG: 1.9653, D(x): 0.8636\n","[e8/10] [b400/938] lossD: 0.8479, lossG: 3.6210, D(x): 0.9477\n","[e8/10] [b500/938] lossD: 0.8967, lossG: 2.4471, D(x): 0.8823\n","[e8/10] [b600/938] lossD: 0.7948, lossG: 2.1229, D(x): 0.8796\n","[e8/10] [b700/938] lossD: 0.7707, lossG: 1.9667, D(x): 0.8610\n","[e8/10] [b800/938] lossD: 0.6986, lossG: 1.9038, D(x): 0.8373\n","[e8/10] [b900/938] lossD: 1.1536, lossG: 2.8727, D(x): 0.9347\n","[e9/10] [b0/938] lossD: 0.7523, lossG: 2.0877, D(x): 0.8177\n","[e9/10] [b100/938] lossD: 0.7437, lossG: 2.1554, D(x): 0.8561\n","[e9/10] [b200/938] lossD: 0.7179, lossG: 1.9164, D(x): 0.7863\n","[e9/10] [b300/938] lossD: 0.6762, lossG: 2.2273, D(x): 0.8795\n","[e9/10] [b400/938] lossD: 0.7051, lossG: 2.4436, D(x): 0.8789\n","[e9/10] [b500/938] lossD: 0.6890, lossG: 2.0203, D(x): 0.8368\n","[e9/10] [b600/938] lossD: 0.7584, lossG: 1.9077, D(x): 0.7455\n","[e9/10] [b700/938] lossD: 0.7065, lossG: 2.0284, D(x): 0.8008\n","[e9/10] [b800/938] lossD: 1.7047, lossG: 0.7754, D(x): 0.2742\n","[e9/10] [b900/938] lossD: 0.6841, lossG: 2.6282, D(x): 0.9090\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nvrPkIu2Bi08"},"source":[""]}]}